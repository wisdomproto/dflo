"""
PDF ê¸°ë°˜ ìë™ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
- PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ìë™ ì²­í¬ ë¶„í•  (ì˜ë¯¸ ë‹¨ìœ„)
- ì„ë² ë”© ìƒì„± ë° Pinecone ì—…ë¡œë“œ
"""

import os
import json
import openai
import pinecone
from pathlib import Path
from typing import List, Dict
import PyPDF2
import re
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
openai.api_key = os.getenv('OPENAI_API_KEY')
pinecone.init(
    api_key=os.getenv('PINECONE_API_KEY'),
    environment='gcp-starter'
)

class PDFtoRAG:
    def __init__(self, pdf_path: str, index_name: str = "growth-clinic-kb"):
        self.pdf_path = pdf_path
        self.index_name = index_name
        self.chunks = []
        
    def extract_text_from_pdf(self) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        print(f"ğŸ“„ PDF íŒŒì¼ ì½ëŠ” ì¤‘: {self.pdf_path}")
        
        text = ""
        with open(self.pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            
            for page_num, page in enumerate(pdf_reader.pages, 1):
                page_text = page.extract_text()
                text += f"\n\n[í˜ì´ì§€ {page_num}]\n{page_text}"
                
                if page_num % 10 == 0:
                    print(f"   ì§„í–‰ë¥ : {page_num}/{total_pages} í˜ì´ì§€")
        
        print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(text):,}ì")
        return text
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ)
        # text = re.sub(r'[^\w\sê°€-í£.,!?Â·\-\(\)\[\]]', '', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def split_into_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì˜¤ë²„ë© í¬í•¨)"""
        print(f"\nğŸ“‹ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ì¤‘ (í¬ê¸°: {chunk_size}, ì˜¤ë²„ë©: {overlap})")
        
        # í˜ì´ì§€ë³„ë¡œ ë¨¼ì € ë¶„í• 
        pages = text.split('[í˜ì´ì§€')
        chunks = []
        chunk_id = 0
        
        for page_text in pages:
            if not page_text.strip():
                continue
            
            # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            page_match = re.match(r'(\d+)\]', page_text)
            page_num = int(page_match.group(1)) if page_match else 0
            
            # ì„¹ì…˜ë³„ë¡œ ë¶„í•  (## ì œëª© ê¸°ì¤€)
            sections = re.split(r'(##\s+[^\n]+)', page_text)
            
            current_section_title = "ë„ì…ë¶€"
            current_text = ""
            
            for i, section in enumerate(sections):
                if section.startswith('##'):
                    # ì´ì „ ì„¹ì…˜ ì €ì¥
                    if current_text.strip():
                        self._create_chunks_from_text(
                            current_text, 
                            current_section_title, 
                            page_num, 
                            chunks, 
                            chunk_size, 
                            overlap
                        )
                    
                    # ìƒˆ ì„¹ì…˜ ì‹œì‘
                    current_section_title = section.strip()
                    current_text = ""
                else:
                    current_text += section
            
            # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
            if current_text.strip():
                self._create_chunks_from_text(
                    current_text, 
                    current_section_title, 
                    page_num, 
                    chunks, 
                    chunk_size, 
                    overlap
                )
        
        print(f"âœ… ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        self.chunks = chunks
        return chunks
    
    def _create_chunks_from_text(self, text: str, title: str, page_num: int, 
                                   chunks: List[Dict], chunk_size: int, overlap: int):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€"""
        text = self.clean_text(text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'([.!?]\s+)', text)
        
        current_chunk = ""
        chunk_count = 0
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')
            
            if len(current_chunk) + len(sentence) > chunk_size:
                if current_chunk:
                    chunks.append({
                        'id': f'chunk_{len(chunks):05d}',
                        'text': current_chunk.strip(),
                        'metadata': {
                            'title': title,
                            'page': page_num,
                            'chunk_index': chunk_count,
                            'source': 'growth_bible_pdf',
                            'type': 'book_content'
                        }
                    })
                    chunk_count += 1
                    
                    # ì˜¤ë²„ë© ì²˜ë¦¬: ë§ˆì§€ë§‰ ë¬¸ì¥ ì¼ë¶€ ìœ ì§€
                    current_chunk = current_chunk[-overlap:] if len(current_chunk) > overlap else ""
            
            current_chunk += sentence
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk.strip():
            chunks.append({
                'id': f'chunk_{len(chunks):05d}',
                'text': current_chunk.strip(),
                'metadata': {
                    'title': title,
                    'page': page_num,
                    'chunk_index': chunk_count,
                    'source': 'growth_bible_pdf',
                    'type': 'book_content'
                }
            })
    
    def create_embeddings_and_upload(self, batch_size: int = 100):
        """ì„ë² ë”© ìƒì„± ë° Pinecone ì—…ë¡œë“œ"""
        print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ ì¤‘...")
        
        # Pinecone ì¸ë±ìŠ¤ ì—°ê²°
        if self.index_name not in pinecone.list_indexes():
            print(f"âš ï¸  ì¸ë±ìŠ¤ '{self.index_name}'ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„± ì¤‘...")
            pinecone.create_index(
                name=self.index_name,
                dimension=1536,
                metric='cosine'
            )
            print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        index = pinecone.Index(self.index_name)
        
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬
        total_chunks = len(self.chunks)
        vectors = []
        
        for i, chunk in enumerate(self.chunks):
            # ì„ë² ë”© ìƒì„±
            try:
                response = openai.embeddings.create(
                    model="text-embedding-3-small",
                    input=chunk['text']
                )
                embedding = response.data[0].embedding
                
                # ë²¡í„° ì¤€ë¹„
                vectors.append({
                    'id': chunk['id'],
                    'values': embedding,
                    'metadata': {
                        **chunk['metadata'],
                        'text_preview': chunk['text'][:200] + '...' if len(chunk['text']) > 200 else chunk['text']
                    }
                })
                
                # ë°°ì¹˜ ì—…ë¡œë“œ
                if len(vectors) >= batch_size or i == total_chunks - 1:
                    index.upsert(vectors=vectors)
                    print(f"   ì—…ë¡œë“œ ì§„í–‰: {i+1}/{total_chunks} ({(i+1)/total_chunks*100:.1f}%)")
                    vectors = []
                
            except Exception as e:
                print(f"âš ï¸  ì²­í¬ {chunk['id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"âœ… ì„ë² ë”© ë° ì—…ë¡œë“œ ì™„ë£Œ!")
        
        # í†µê³„ í™•ì¸
        stats = index.describe_index_stats()
        print(f"\nğŸ“Š Pinecone í†µê³„:")
        print(f"   - ì´ ë²¡í„° ìˆ˜: {stats['total_vector_count']:,}")
        print(f"   - ì¸ë±ìŠ¤ ì°¨ì›: {stats['dimension']}")
    
    def save_chunks_to_json(self, output_path: str = "data/processed/chunks.json"):
        """ì²­í¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë°±ì—…ìš©)"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì²­í¬ ë°±ì—… ì™„ë£Œ: {output_path}")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ PDF â†’ RAG ìë™ êµ¬ì¶• ì‹œì‘!")
        print("=" * 60)
        
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf()
        
        # 2. ì²­í¬ ë¶„í• 
        chunks = self.split_into_chunks(text, chunk_size=1000, overlap=200)
        
        # 3. JSON ë°±ì—…
        self.save_chunks_to_json()
        
        # 4. ì„ë² ë”© ë° ì—…ë¡œë“œ
        self.create_embeddings_and_upload()
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 60)
        print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
        print(f"   - ì´ ì²­í¬ ìˆ˜: {len(self.chunks):,}")
        print(f"   - í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(c['text']) for c in self.chunks) / len(self.chunks):.0f}ì")
        print(f"   - Pinecone ì¸ë±ìŠ¤: {self.index_name}")
        print(f"\nğŸ‰ ì´ì œ AI ì±—ë´‡ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    # PDF íŒŒì¼ ê²½ë¡œ
    pdf_path = "ìš°ë¦¬ ì•„ì´ í‚¤ ì„±ì¥ ë°”ì´ë¸” ì›ê³ .pdf"
    
    # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    rag_builder = PDFtoRAG(pdf_path, index_name="growth-clinic-kb")
    rag_builder.run()
    
    print("\n" + "=" * 60)
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("1. Cloudflare Worker ë°°í¬ (cloudflare-worker-rag.js)")
    print("2. ì›¹ì•±ì— AI ì±—ë´‡ UI ì¶”ê°€ (js/ai-growth-consultant.js)")
    print("3. í…ŒìŠ¤íŠ¸: 'ì•„ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?'")
    print("=" * 60)
